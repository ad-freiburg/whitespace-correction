experiment:
  name: env(EXPERIMENT_NAME:multi_byte_large)

seed: 22

input_tokenizer: file(tokenizers/byte_multi.yaml)

model: file(models/byte_transformer_encoder.yaml)

train:
  load_checkpoint: env(LOAD_CHECKPOINT)
  mixed_precision: env(MIXED_PRECISION:true)
  mixed_precision_dtype: env(MIXED_PRECISION_DTYPE:fp16)
  clip_grad_norm: env(CLIP_GRAD_NORM:1.0)
  num_epochs: env(NUM_EPOCHS:1)
  eval_interval: eval(1 / env(EVAL_PER_EPOCH:20))
  log_interval: eval(1 / env(LOG_PER_EPOCH:1000))
  loss: file(losses/env(LOSS:focal).yaml)
  optimizer:
    type: adamw
    lr: eval(env(LR:0.0001) * 0.1)
    weight_decay: 0.01
    param_groups:
      - prefix: head
        lr: env(LR:0.0001)
  lr_scheduler:
    type: cosine_with_warmup
    warmup_steps: env(WARMUP_STEPS:0.01)
  metrics:
    whitespace_correction:
      max_items: 8
      multi_layer: env(MULTI_LAYER:false)
  data:
    strategy: weighted
    shuffle: true
    sort: true
    limit: env(TRAIN_LIMIT:null)
    max_length: env(MAX_LENGTH:512)
    # max_length: eval(env(MAX_LENGTH:512) // 4)
    # max_length_scheduler:
    #   type: multi_step
    #   steps: [0.75, 0.90]
    #   factors: [2.0, 2.0]
    buffer_size: env(BUFFER_SIZE:512)
    prefetch_factor: env(PREFETCH_FACTOR:512)
    num_threads: eval(env(THREADS:None) or len(os.sched_getaffinity(0)) // 2)
    batch_limit: eval(env(MAX_LENGTH:512) * env(BATCH_LIMIT:32))
    batch_limit_type: padded_item_size
    default_language: (lang:unk)
    pipeline:
      preprocessing: file(preprocessings/whitespace_corruption_byte_multi.yaml)
      tokenizer: file(tokenizers/byte_multi.yaml)
      labeling:
        type: whitespace_correction
        tokenizer: file(tokenizers/byte_multi.yaml)
      postprocessing:
        type: none
    sources: file(env(DATA_SOURCES:en_arxiv_data_sources.yaml))
    val: env(VAL_LIMIT:10000)
