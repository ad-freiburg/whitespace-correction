experiment: "${MODEL_NAME}_whitespace_correction_byte"
experiment_dir: ${EXPERIMENT_DIR}

seed: 22

train:
  num_epochs: ${NUM_EPOCHS:1}
  train_data: ${LMDB_PATH}

  in_memory: false
  num_workers: 0

  batch_max_tokens: ${BATCH_MAX_TOKENS:65536}
  max_seq_length: ${MAX_SEQ_LENGTH:512}

  optimizer:
    type: "adamw"
    learning_rate: ${LR:0.0001}
    weight_decay: 0.01

  lr_scheduler:
    type: "cosine_with_warmup"
    arguments:
      warmup_steps: ${WARMUP_STEPS:16000}

  loss:
    type: "seq2seq_cross_entropy"
    arguments:
      ignore_index: -1
      weights: [1, 5, 5]

  swap_inputs_and_targets: false
  mixed_precision: true
  eval_interval: ${EVAL_INTERVAL:0.25}
  log_interval: ${LOG_INTERVAL:0.01}
  keep_last_n_checkpoints: 1

val:
  val_data: 5000

  text_metrics:
    - name: "qualitative_batch_evaluation_whitespace_correction"

model:
  name: "${MODEL_NAME}"
  type: "transformer_encoder_with_head"

  pretrained: ${PRETRAINED:null}

  encoder:
    tokenizer: "byte"

    pretrained: ${ENCODER_PRETRAINED:null}

    embedding_dim: ${MODEL_DIM:512}
    max_num_embeddings: ${MAX_SEQ_LENGTH:512}
    learned_positional_embeddings: false
    norm_embeddings: true
    model_dim: ${MODEL_DIM:512}
    feedforward_dim: ${FFN_DIM:2048}
    attention_heads: ${NUM_HEADS:8}
    dropout: 0.1

    num_layers: ${NUM_LAYERS:6}
    share_parameters: false
    activation: "gelu"

    # determines how byte representations are grouped to character representations
    group_name: char_groups
    group_aggregation: mean
    group_at: after

  head:
    type: "sequence_classification"
    arguments:
      model_dim: ${MODEL_DIM:512}
      num_layers: ${HEAD_NUM_LAYERS:2}
      num_classes: 3
