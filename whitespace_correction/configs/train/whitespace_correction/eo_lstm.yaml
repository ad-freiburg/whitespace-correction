experiment: "${MODEL_NAME}_whitespace_correction_char"
experiment_dir: ${EXPERIMENT_DIR}

seed: 22

train:
  num_epochs: ${NUM_EPOCHS:1}
  train_data: ${LMDB_PATH}

  in_memory: false
  num_workers: 0

  batch_max_tokens: ${BATCH_MAX_TOKENS:65536}
  max_seq_length: ${MAX_SEQ_LENGTH:512}

  optimizer:
    type: "adamw"
    learning_rate: ${LR:0.0001}
    weight_decay: 0.01

  lr_scheduler:
    type: "cosine_with_warmup"
    arguments:
      warmup_steps: ${WARMUP_STEPS:16000}

  loss:
    type: "seq2seq_cross_entropy"
    arguments:
      ignore_index: -1
      weights: [1, 5, 5]

  swap_inputs_and_targets: false
  mixed_precision: true
  eval_interval: ${EVAL_INTERVAL:0.25}
  log_interval: ${LOG_INTERVAL:0.01}
  keep_last_n_checkpoints: 1

val:
  val_data: 5000

  text_metrics:
    - name: "sequence_classification"

model:
  name: "${MODEL_NAME}"
  type: "rnn_encoder_with_head"

  pretrained: ${PRETRAINED:null}

  encoder:
    tokenizer: "char"

    pretrained: ${ENCODER_PRETRAINED:null}

    embedding_dim: ${MODEL_DIM:512}
    norm_embeddings: false
    model_dim: ${MODEL_DIM:512}
    dropout: 0.1

    type: lstm
    bidirectional: true
    num_layers: ${NUM_LAYERS}

  head:
    type: "sequence_classification"
    arguments:
      model_dim: ${MODEL_DIM:512}
      num_layers: ${HEAD_NUM_LAYERS:1}
      num_classes: 3

