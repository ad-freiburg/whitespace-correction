experiment:
  name: eo_medium_lstm_byte_focal
  dir: abspath(env(EXPERIMENT_DIR:experiments))

seed: 22

input_tokenizer: file(tokenizers/byte.yaml)

model:
  type: encoder_with_head
  embedding:
    embedding_dim: env(DIM:512)
    dropout: 0.0
    positional_embeddings: sinusoidal
    max_length: env(MAX_LENGTH:512)
  encoder:
    type: grouping
    group_first: env(GROUP_FIRST:true)
    group_aggregation: mean
    encoder:
      type: rnn
      rnn_type: lstm
      dim: env(DIM:512)
      num_layers: env(NUM_LAYERS:3)
      dropout: 0.1
  head:
    type: sequence_classification
    dim: env(DIM:512)
    num_layers: 2
    dropout: 0.1
    num_classes: 3

train:
  mixed_precision: true
  num_epochs: 3
  eval_interval: eval(1 / env(EVAL_PER_EPOCH:5))
  log_interval: eval(1 / env(LOG_PER_EPOCH:100))
  loss:
    type: sequence_focal
    weights: [1, 10, 10]
    ignore_index: -1
    gamma: 2
    gamma_schedule: cosine
  optimizer:
    type: adamw
    lr: env(LR:0.0001)
    weight_decay: 0.001
  lr_scheduler:
    type: cosine_with_warmup
    warmup_steps: 0.01
  metrics:
    whitespace_correction:
      max_items: 4
  data:
    strategy: weighted
    shuffle: true
    sort: true
    buffer_size: eval(env(BATCH_LIMIT:32) * env(BATCH_LIMIT:32))
    prefetch_factor: env(BATCH_LIMIT:32) 
    num_threads: 4
    batch_limit: eval(env(MAX_LENGTH:512) * env(BATCH_LIMIT:32))
    batch_limit_type: env(BATCH_LIMIT_TYPE:padded_item_size)
    pipeline:
      preprocessing: file(preprocessings/whitespace_noise_byte.yaml)
      labeling:
        type: whitespace_correction
      tokenizer: file(tokenizers/byte.yaml)
    sources:
      - type: file
        path: abspath(env(DATA_FILE))
        language: en
    val: env(VAL_LIMIT:10000)
