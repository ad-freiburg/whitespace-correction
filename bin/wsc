#!/usr/bin/env python

import argparse
import logging
import sys
import time

import torch
import torch.backends.cudnn

import whitespace_correction
from whitespace_correction import WhitespaceCorrector
from whitespace_correction.api.utils import generate_report


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        "Whitespace correction using Transformers",
        description="Correct whitespaces in text by inserting missing or deleting superfluous whitespaces"
    )
    parser.add_argument(
        "-m",
        "--model",
        choices=[model.name for model in WhitespaceCorrector.available_models()],
        default=WhitespaceCorrector.available_models()[0].name,
        help="Name of the model to use for whitespace correction"
    )
    input_group = parser.add_mutually_exclusive_group()
    input_group.add_argument(
        "-c",
        "--correct",
        type=str,
        default=None,
        help="Text to correct"
    )
    input_group.add_argument(
        "-f",
        "--file",
        type=str,
        default=None,
        help="Path to a text file which will be corrected line by line"
    )
    input_group.add_argument(
        "-i",
        "--interactive",
        action="store_true",
        default=None,
        help="Start an interactive session where your command line input is corrected"
    )
    parser.add_argument(
        "-o",
        "--out-path",
        type=str,
        default=None,
        help="Path where corrected text should be saved to"
    )
    parser.add_argument(
        "--cpu",
        action="store_true",
        help="Force to run the model on CPU, by default a GPU is used if available"
    )
    parser.add_argument(
        "--num-threads",
        type=int,
        default=None,
        help="Number of threads used for running the inference pipeline"
    )
    batch_limit_group = parser.add_mutually_exclusive_group()
    batch_limit_group.add_argument(
        "-b",
        "--batch-size",
        type=int,
        default=16,
        help="Determines how many inputs will be corrected at the same time, larger values should usually result in "
             "faster correcting but require more memory"
    )
    batch_limit_group.add_argument(
        "-t",
        "--batch-max-tokens",
        type=int,
        default=None,
        help="Determines the maximum number of tokens corrected at the same time"
    )
    parser.add_argument(
        "-u",
        "--unsorted",
        action="store_true",
        help="Disable sorting of the inputs before correcting (for a large number of inputs or large text files "
             "sorting the sequences beforehand leads to speed ups because it minimizes the amount of padding needed "
             "within a batch of sequences)"
    )
    parser.add_argument(
        "-e",
        "--experiment",
        type=str,
        default=None,
        help="Path to an experiment directory from which the model will be loaded "
             "(use this when you trained your own model and want to use it)"
    )
    parser.add_argument(
        "-l",
        "--list",
        action="store_true",
        help="List all available models with short descriptions"
    )
    parser.add_argument(
        "--progress",
        action="store_true",
        help="Show a progress bar (this flag is only respected when getting input from stdin, "
             "in interactive mode with -i progress is never shown, "
             "when correcting a file with -f progress is always shown)"
    )
    parser.add_argument(
        "-p",
        "--pipe",
        action="store_true",
        help="Pass this flag when using whitespace correction in a pipe because input and output is then treated as an "
             "iterator (note that sorting by length gets disabled with this flag because it is not possible to sort an "
             "iterator)"
    )
    parser.add_argument(
        "--precision",
        choices=["fp32", "fp16", "bfp16"],
        default="fp32",
        help="Choose the precision for inference, fp16 or bfp16 can result in faster runtimes when running on a "
             "new GPU that supports lower precision, but it can be slower on older GPUs."
    )
    parser.add_argument(
        "-v",
        "--version",
        action="store_true",
        help="Print the version of the whitespace_correction library"
    )
    parser.add_argument(
        "--force-download",
        action="store_true",
        help="Download the model again even if it already was downloaded"
    )
    parser.add_argument(
        "--server",
        type=str,
        default=None,
        help="JSON config file to run a whitespace correction server"
    )
    parser.add_argument(
        "--report",
        type=str,
        default=None,
        help="Save a runtime report (ignoring startup time) formatted as markdown table to a file, append new line "
             "if the file already exists"
    )
    parser.add_argument(
        "--log-level",
        type=str,
        choices=["none", "info", "debug"],
        default="none",
        help="Sets the logging level for the underlying loggers"
    )
    return parser.parse_args()


def run(args: argparse.Namespace) -> None:
    if args.version:
        print(f"whitespace correction version {whitespace_correction.__version__}")
        return
    elif args.list:
        model_str = "\n".join(
            f"- {model.name}: {model.description}" for model in WhitespaceCorrector.available_models())
        print(f"available models:\n{model_str}")
        return
    elif args.server is not None:
        from whitespace_correction.api.server import run_flask_server
        run_flask_server(args.server)
        return

    if args.log_level == "info":
        logging.basicConfig(level=logging.INFO)
    elif args.log_level == "debug":
        logging.basicConfig(level=logging.DEBUG)
    else:
        logging.disable(logging.CRITICAL)

    if args.experiment:
        ws_cor = WhitespaceCorrector.from_experiment(
            experiment_dir=args.experiment,
            device="cpu" if args.cpu else "cuda"
        )
    else:
        ws_cor = WhitespaceCorrector.from_pretrained(
            model=args.model,
            device="cpu" if args.cpu else "cuda",
            force_download=args.force_download
        )

    ws_cor: WhitespaceCorrector
    ws_cor.set_precision(args.precision)

    if ws_cor.device.type == "cuda":
        torch.cuda.reset_peak_memory_stats(ws_cor.device)
    start = time.perf_counter()
    if args.correct is not None:
        print(ws_cor.correct_text(args.correct))
    elif args.file is not None:
        corrected_lines = ws_cor.correct_file(
            input_file_path=args.file,
            output_file_path=args.out_path,
            batch_size=args.batch_size,
            batch_max_tokens=args.batch_max_tokens,
            sort=not args.unsorted,
            num_threads=args.num_threads
        )
        if args.out_path is None:
            for line in corrected_lines:
                print(line)
        if args.report:
            with open(args.file, "r", encoding="utf8") as inf:
                lines = [line.strip() for line in inf]

            generate_report(
                "whitespace correction",
                ws_cor.name,
                ws_cor.model,
                lines,
                time.perf_counter() - start,
                ws_cor._mixed_precision_dtype,
                args.batch_size,
                not args.unsorted,
                ws_cor.device,
                file_path=args.report
            )

    elif args.interactive:
        while True:
            try:
                line = input()
                print(ws_cor.correct_text(line))
            except KeyboardInterrupt:
                return
    else:
        if sys.stdin.isatty():
            return

        try:
            if args.pipe:
                # correct lines from stdin as they come
                for line in sys.stdin:
                    print(
                        ws_cor.correct_text(
                            inputs=line.strip(),
                            sort=False
                        )
                    )
            else:
                # read stdin completely, then potentially sort and correct
                lines = [line.strip() for line in sys.stdin]
                for corrected_line in ws_cor.correct_text(
                        inputs=lines,
                        batch_size=args.batch_size,
                        batch_max_tokens=args.batch_max_tokens,
                        sort=not args.unsorted,
                        num_threads=args.num_threads
                ):
                    print(corrected_line)

                if args.report:
                    generate_report(
                        "whitespace correction",
                        ws_cor.name,
                        ws_cor.model,
                        lines,
                        time.perf_counter() - start,
                        ws_cor._mixed_precision_dtype,
                        args.batch_size,
                        not args.unsorted,
                        ws_cor.device,
                        file_path=args.report
                    )

        except BrokenPipeError:
            return
        except Exception as e:
            raise e


if __name__ == "__main__":
    run(parse_args())
