#!/usr/bin/env python

import argparse
import logging
import os
import sys
import time

import torch
import torch.backends.cudnn

import trt
from trt import get_available_models, TokenizationRepairer
from trt.api.utils import generate_report


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        "Tokenization repair using Transformers",
        description="Repair tokenization in text by inserting missing or deleting superfluous whitespaces"
    )
    parser.add_argument(
        "-m",
        "--model",
        choices=[model.name for model in get_available_models()],
        default=get_available_models()[0].name,
        help="Name of the model to use for tokenization repair"
    )
    input_group = parser.add_mutually_exclusive_group()
    input_group.add_argument(
        "-r",
        "--repair",
        type=str,
        default=None,
        help="Text to repair"
    )
    input_group.add_argument(
        "-f",
        "--file",
        type=str,
        default=None,
        help="Path to a text file which will be repaired line by line"
    )
    input_group.add_argument(
        "-i",
        "--interactive",
        action="store_true",
        default=None,
        help="Start an interactive session where your command line input is repaired"
    )
    parser.add_argument(
        "-o",
        "--out-path",
        type=str,
        default=None,
        help="Path where repaired text should be saved to"
    )
    parser.add_argument(
        "--cpu",
        action="store_true",
        help="Force to run the model on CPU, by default a GPU is used if available"
    )
    parser.add_argument(
        "-b",
        "--batch-size",
        type=int,
        default=16,
        help="Determines how many inputs will be repaired at the same time, larger values should usually result in "
             "faster repairing but require more memory"
    )
    parser.add_argument(
        "-u",
        "--unsorted",
        action="store_true",
        help="Disable sorting of the inputs before repairing (for a large number of inputs or large text files sorting "
             "the sequences beforehand leads to speed ups because it minimizes the amount of padding needed "
             "within a batch of sequences)"
    )
    parser.add_argument(
        "-e",
        "--experiment",
        type=str,
        default=None,
        help="Path to an experiment directory from which the model will be loaded "
             "(use this when you trained your own model and want to use it)"
    )
    parser.add_argument(
        "-l",
        "--list",
        action="store_true",
        help="List all available models with short descriptions"
    )
    parser.add_argument(
        "--progress",
        action="store_true",
        help="Show a progress bar (this flag is only respected when getting input from stdin, "
             "in interactive mode with -i progress is never shown, "
             "when repairing a file with -f progress is always shown)"
    )
    parser.add_argument(
        "-p",
        "--pipe",
        action="store_true",
        help="Pass this flag when using trt in a pipe because input and output is then treated as an iterator "
             "(note that sorting by length gets disabled with this flag because it is not possible to sort an "
             "iterator)"
    )
    parser.add_argument(
        "-v",
        "--version",
        action="store_true",
        help="Print the version of the trt library"
    )
    parser.add_argument(
        "--force-download",
        action="store_true",
        help="Download the model again even if it already was downloaded"
    )
    parser.add_argument(
        "--server",
        type=str,
        default=None,
        help="Run a tokenization repair server, specify host and port in the form <host>:<port>"
    )
    parser.add_argument(
        "--report",
        type=str,
        default=None,
        help="Save a runtime report (ignoring startup time) formatted as markdown table to a file, append new line "
             "if file already exists"
    )
    return parser.parse_args()


def run(args: argparse.Namespace) -> None:
    torch.backends.cudnn.benchmark = True
    torch.set_num_threads(len(os.sched_getaffinity(0)))
    torch.use_deterministic_algorithms(False)

    if args.version:
        print(f"trt version {trt.__version__}")
        return
    elif args.list:
        model_str = "\n".join(f"- {model.name}: {model.description}" for model in get_available_models())
        print(f"Available models:\n{model_str}")
        return
    elif args.server is not None:
        try:
            host, port = args.server.split(":")
            port = int(port)
        except Exception:
            raise RuntimeError(f"could not parse host and port from {args.server}")

        from trt.api.server import run_flask_server
        run_flask_server(host, port)
        return

    # disable logging here since we do not want that for our command line interface, but for the server above
    logging.disable(logging.CRITICAL)

    if args.experiment:
        tok_rep = TokenizationRepairer.from_experiment(
            experiment_dir=args.experiment,
            device="cpu" if args.cpu else "cuda"
        )
    else:
        tok_rep = TokenizationRepairer.from_pretrained(
            model=args.model,
            device="cpu" if args.cpu else "cuda",
            force_download=args.force_download
        )

    start = time.perf_counter()
    if args.repair is not None:
        print(tok_rep.repair_text(args.repair))
    elif args.file is not None:
        repaired_lines = tok_rep.repair_file(
            input_file_path=args.file,
            output_file_path=args.out_path,
            batch_size=args.batch_size,
            sort_by_length=not args.unsorted
        )
        if args.out_path is None:
            for line in repaired_lines:
                print(line)
        if args.report:
            with open(args.file, "r", encoding="utf8") as inf:
                lines = [line.strip() for line in inf]

            generate_report(
                "tokenization repair",
                tok_rep.model_name,
                lines,
                time.perf_counter() - start,
                args.batch_size,
                not args.unsorted,
                tok_rep.device,
                file_path=args.report
            )
    elif args.interactive:
        while True:
            try:
                line = input()
                print(tok_rep.repair_text(line))
            except KeyboardInterrupt:
                return
    else:
        if sys.stdin.isatty():
            return

        try:
            if args.pipe:
                # repair lines from stdin as they come
                for line in sys.stdin:
                    print(
                        tok_rep.repair_text(
                            inputs=line.strip(),
                            sort_by_length=False
                        )
                    )
            else:
                # read stdin completely, then potentially sort and repair
                lines = [line.strip() for line in sys.stdin]
                for repaired_line in tok_rep.repair_text(
                        inputs=lines,
                        batch_size=args.batch_size,
                        sort_by_length=not args.unsorted,
                        show_progress=args.progress
                ):
                    print(repaired_line)
                if args.report:
                    generate_report(
                        "tokenization repair",
                        tok_rep.model_name,
                        lines,
                        time.perf_counter() - start,
                        args.batch_size,
                        not args.unsorted,
                        tok_rep.device,
                        file_path=args.report
                    )
        except BrokenPipeError:
            return
        except Exception as e:
            raise e


if __name__ == "__main__":
    run(parse_args())
