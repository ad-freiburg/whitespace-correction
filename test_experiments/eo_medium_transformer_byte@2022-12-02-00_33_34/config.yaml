experiment:
  dir: /home/sebastian/phd/whitespace_correction/code/test_experiments
  name: eo_medium_transformer_byte
input_tokenizer:
  default_prefix_tokens:
  - <bos>
  default_suffix_tokens:
  - <eos>
  type: byte
model:
  embedding:
    dropout: 0.1
    embedding_dim: 512
    max_length: 512
    positional_embeddings: sinusoidal
  encoder:
    dim: 512
    dropout: 0.1
    ffw_dim: 2048
    heads: 8
    num_layers: 6
    type: transformer
    with_pos: true
  head:
    dim: 512
    dropout: 0.1
    num_classes: 3
    num_layers: 2
    type: sequence_classification
  type: encoder_with_head
seed: 22
train:
  data:
    batch_limit: 1024
    batch_limit_type: num_tokens
    num_threads: 4
    pipeline:
      labeling:
        type: whitespace_correction
      preprocessing: file(preprocessings/whitespace_noise.yaml)
      tokenizer: file(tokenizers/byte.yaml)
    shuffle: true
    shuffle_prefetch_factor: 16
    sources:
    - language: en
      path: /home/sebastian/phd/whitespace_correction/code/30K
      type: file
    strategy: weighted
    val: 10000
  loss:
    ignore_index: -1
    num_classes: 3
    type: sequence_cross_entropy
    weights:
    - 1
    - 10
    - 10
  lr_schedule:
    type: cosine_with_warmup
  mixed_precision: true
  num_epochs: 3
  optimizer:
    lr: 0.0001
    type: adamw
    weight_decay: 0.001
