experiment:
  dir: /home/sebastian/phd/whitespace_correction/code/test_experiments
  name: eo_medium_transformer_byte
input_tokenizer:
  default_prefix_tokens:
  - <bos>
  default_suffix_tokens:
  - <eos>
  type: byte
model:
  embedding:
    dropout: 0.1
    embedding_dim: 512
    max_length: 512
    positional_embeddings: sinusoidal
  encoder:
    dim: 512
    dropout: 0.1
    ffw_dim: 2048
    heads: 8
    num_layers: 6
    type: transformer
    with_pos: true
  head:
    dim: 512
    dropout: 0.1
    num_classes: 3
    num_layers: 2
    type: sequence_classification
  type: encoder_with_head
seed: 22
train:
  data:
    batch_limit: 1024
    batch_limit_type: num_tokens
    num_threads: 4
    pipeline:
      labeling:
        type: whitespace_correction
      preprocessing:
      - type: clean
      - type: overwrite
      - configs:
        - delete_whitespace_prob: 0.1
          insert_whitespace_prob: 0.1
          type: noise_whitespaces
        - type: full_whitespaces
        - type: no_whitespaces
        probabilities:
        - 0.8
        - 0.1
        - 0.1
        type: switch
      tokenizer:
        default_prefix_tokens:
        - <bos>
        default_suffix_tokens:
        - <eos>
        type: byte
    shuffle: true
    shuffle_prefetch_factor: 16
    sources:
    - language: en
      path: /home/sebastian/phd/whitespace_correction/code/whitespace_correction/data/downloads/multi30k.txt
      type: file
    strategy: weighted
    val: 10000
  eval_interval: 0.5
  log_interval: 0.01
  loss:
    ignore_index: -1
    num_classes: 3
    type: sequence_cross_entropy
    weights:
    - 1
    - 10
    - 10
  lr_schedule:
    type: cosine_with_warmup
  mixed_precision: true
  num_epochs: 3
  optimizer:
    lr: 0.0001
    type: adamw
    weight_decay: 0.001
